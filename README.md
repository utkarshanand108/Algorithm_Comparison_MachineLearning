# DataScienceCourse6_Assignment1
**Algorithm Comparison in Machine Learning â€” DS PGC Course 6 Assignment 1**

---

## ðŸ“˜ Assignment Overview
This assignment explores **four core Machine Learning algorithms** â€” Logistic Regression, K-Nearest Neighbors (KNN), Decision Tree, and Support Vector Machine (SVM).  
It explains how each algorithm works, their strengths and limitations, and compares their suitability across various real-world dataset scenarios.

---

## ðŸ§© Tasks Summary
**Part 1 â€” Algorithm Overviews**
- Logistic Regression â€” interpretable, probabilistic classifier assuming linear separability.  
- K-Nearest Neighbors â€” instance-based learner that predicts based on local similarity.  
- Decision Tree â€” rule-based, interpretable model handling non-linear patterns.  
- Support Vector Machine â€” margin-maximizing classifier, powerful in high-dimensional data.

**Part 2 â€” Application Scenarios**
1. **High-Dimensional Data (Text/Gene)** â†’ *SVM (Linear Kernel)*  
2. **Imbalanced Dataset (Fraud/Rare Disease)** â†’ *Logistic Regression (Class Weights)*  
3. **Small Dataset with Many Features** â†’ *SVM (Regularized Linear Kernel)*  
4. **Non-Linear Data Separation (Spirals/Circles)** â†’ *SVM (RBF Kernel)*  
5. **Noisy Data / Many Irrelevant Features** â†’ *Logistic Regression (L1 Regularization)*  

---

## ðŸ§° Tools & Concepts Used
- **ML Algorithms:** Logistic Regression, KNN, Decision Tree, SVM  
- **Core Concepts:** Regularization (L1/L2), Kernel Trick, Bias-Variance, Margin Optimization  
- **Implementation Environments:** Jupyter Notebook / Google Colab  
- **Libraries (if coded):** `scikit-learn`, `pandas`, `numpy`, `matplotlib`, `seaborn`

---

## ðŸ“‚ Files Included
- `DataScienceCourse6Assignment1.pdf` â€” problem statement  
- `DataScienceCourse6Solution1.pdf` â€” detailed written solution and comparative analysis  

---

## ðŸ§­ Key Learnings
- Logistic Regression and SVM perform best on structured, high-dimensional, or imbalanced data.  
- KNN is effective for simple, low-dimensional, balanced datasets but not scalable.  
- Decision Trees excel in interpretability but risk overfitting without pruning.  
- SVMâ€™s kernel flexibility handles non-linear separations, outperforming others in complex spaces.  

---

## ðŸ‘¤ Author
**Utkarsh Anand** â€” DS PGC Course 6 Assignment 1  
Internshala Placement Guarantee Program
